{"posts":[{"title":"Introduction to Plasma","text":"Our Universe is made of 69% dark energy, 27% dark matter, 1% normal matter. What is plasma?Plasma is also called the “fourth state of matter”. Solid is heated to become a liquid, liquid is heated to become a gas.Upon further heating, the gas is ionized into a plasma. Since plasma usually exists only in a vacuum, we need to pump the air out of a vacuum chamber in the laboratory. The Definition of Plasma A plasma is a quasineutral gas of charged and neutral particles which exhibits collective behavior. The Coulomb force between A and B diminishes as 1/r^2. However, for a given solid angle($\\Delta$r/r = constant), the volume of plasma in B that can affect A increases as r^3. The Saha Equation\\frac{n_i}{n_n}\\approx2.4\\times10^{21}\\frac{T^{3/2}}{n_i}e^{-U_i/KT}n_i\\ is\\ the\\ density\\ of\\ ionized\\ atomsn_n\\ is\\ the\\ density\\ of\\ neutral\\ atomsK\\ is\\ Boltzmann's\\ constantU_i\\ is\\ the\\ ionization\\ energy\\ of\\ the\\ gasPhysical meaning When temperature is raising, the whole value is increasing exponentially with U_i/KT. The higher value of n_i, the lower recombination rate of ionized atoms. The Maxwellian DistributionThe one-dimensional Maxwellian distributionf(u)=Aexp(-\\frac{1}{2}mu^2/KT)fdu\\ is\\ the\\ number\\ of\\ particles\\ per\\ m^3\\ with\\ velocity\\ between\\ u\\ and\\ u+du\\frac{1}{2}mu^2\\ is\\ the\\ kinetic\\ energyBoltzmann’s constant KK=1.38\\times10^{-23}J/^{\\circ}KThe particles density nn=\\int_{-\\infty}^{\\infty}f(u)duThe constant A is related to the density nA=n(\\frac{m}{2\\pi KT})^{1/2} The average kinetic energy is $\\frac{1}{2}KT$ The three-dimensional Maxwellian distributionf(u,v,w)=A_3exp[-\\frac{1}{2}m(u^2+v^2+w^2)/KT] Reference BookIntroduction to Plasma Physics and Controlled Fusion by Francis F. Chen","link":"/2023/01/12/Introduction%20to%20Plasma/"},{"title":"Anaconda Command","text":"Anaconda Prompt OperationCreate new environment conda create - -name environment pakage conda create - -name python3 python=3.8 Activate environment conda activate environment Exit environment deactivate environment Delete environment conda remove -n environment - -all Copy environment conda create -n environment - -clone existing_environment View environment information conda info -econda env listconda info - -envs View python version python -V Install package conda install package View package information conda search package Install package in a specify environment conda install -n environment package Update package in a specify environment conda update -n environment package Delete package in a specify environment conda remove -n environment package View the installed packages in the current environment conda list View the installed packages in a specify environment conda list -n environment","link":"/2023/01/12/Anaconda%20Command/"},{"title":"Image Formation","text":"2D points\\mathbf x=(x,y)\\in\\textit R^2\\mathbf x=\\begin{bmatrix} x \\\\ y \\end{bmatrix}homogeneous vector \\mathbf{\\tilde x}\\mathbf{\\tilde x}=(\\tilde x,\\tilde y,\\tilde w)\\in\\textit P^2 2D projective space \\textit P^2=\\textit R^3-(0,0,0) inhomogeneous vector \\mathbf{\\overline x}\\mathbf{\\tilde x}=\\tilde w(x,y,1)=\\tilde w\\mathbf{\\overline x}2D lineshomogeneous vector\\tilde{\\mathbf{\\textit l}}=(a,b,c)line equation\\mathbf{\\overline{x}}\\cdot\\tilde{\\mathbf{\\textit l}}=ax+by+c=0 \\mathbf{\\textit l}=(\\hat n_x,\\hat n_y,d)=(\\hat{\\mathbf n},d)with\\Vert{\\hat{\\mathbf n}}\\Vert=1Reference BookComputer Vision: Algorithms and Applications by Richard Szeliski","link":"/2023/01/13/Image%20Formation/"},{"title":"GNN Paper","text":"Review Graph Neural Networks: A Review of Methods and Applications, 2018 Deep Learning on Graphs: A Survey, 2018 Relationanl Inductive Biases, Deep Learning, and Graph Neural Networks, 2018 Geometric Deep Learning: Going beyond Euclidean data, 2017 Computational Capabilities of Graph Neural Networks, 2009 Neural Message Passing for Quantum Chemistry, 2017 Non-local Neural Networks, 2018 The Graph Neural Network Model, 2009 Model A new model for learning in graph domains, 2005 Graph Neural Networks for Ranking Web Pages, 2005 Gated Graph Sequence Neural Networks, 2015 Geometric deep learning on graphs and manifolds using mixture model CNNs, 2016 Spectral Networks and locally Connected Networks on Graphs, 2013 Deep Convolutional Networks on Graph-Structure Data, 2015 Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, 2016 Learning Convolutional Neural Networks for Graphs, 2016 Semi-Supervised Classification with Graph Convolutional Networks, 2016 Graph Attention Networks, 2017 Deep Sets, 2017 Graph Partition Neural Networks for Semi-Supervised Classification, 2018 Covariant Compositional Networks For Learning Graphs, 2018 Modeling Relational Data with Graph Convolutional Networks, 2018 Stochastic Training of Graph Convolutional Networks with Variance Reduction, 2018 Learning Steady-States of Iterative Algorithms over Graphs, 2018 Deriving Neural Architectures from Sequence and Graph Kernels, 2017 Adaptive Graph Convolutional Neural Networks, 2018 Graph-to-Sequence Learning using Gated Graph Neural Networks, 2018 Deeper insights into Graph Convolutional Networks for Semi-Supervised Learning, 2018 -Graphical-Based Learning Environments for Pattern Recognition, 2004 A Comparison between Recursive Neural Networks and Graph Neural Networks, 2006 Graph Neural Networks for Object Localization, 2006 Knowledge-Guided Recurrent Neural Network Learning for Task-Oriented Action Prediction, 2017 Semantic Object Parsing with Graph LSTM, 2016 CelebrityNet: A Social Network Constructed from Large-Scale Online Celebrity Images, 2015 Inductive Representation Learning on Large Graphs, 2017 Graph Classification using Structural Attention, 2018 Adversarial Attacks on Neural Networks for Graph Data, 2018 Large-Scale Learnable Graph Convolutional Networks, 2018 Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing, 2018 -Diffusion-Convolutional Neural Networks, 2016 Neural networks for relational learning: an experimental comparison, 2011 FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling, 2018 Adaptive Sampling Towards Fast Graph Representation Learning, 2018 Application Discovering objects and their relations from entanglrd scene representations, 2017 A simple neural network module for relational reasoning, 2017 Attend, Infer, Repeat: Fast Scene Understanding with Generative Models Beyond Categories: The Visual Memex Model for Reasoning About Object Relationships Understanding Kin Relationships in a Photo Graph-Structured Representations for Visual Question Answering Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition Few-Shot Learning with Graph Neural Networks The More You Know: Using Knowledge Graphs for Image Classification Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs Rethinking Knowledge Graph Propagation for Zero-Shot Learning Interaction Networks for Learning about Objects, Relations and Physics A Compositional Object-Based Approach to Learning Physical Dynamics Visual Interaction Networks: Learning a Physics Simulator from Video Relational neural expectation maximization: Unsupervised discovery of objects and their interactions Graph networks as learnable physics engines for inference and control Learning Multiagent Communication with Backpropagation VAIN: Attentional Multi-agent Predictive Modeling Neural Relational Inference for Interacting Systems Translating Embeddings for Modeling Multi-relational Data","link":"/2023/01/14/GNN%20Paper/"},{"title":"Graph Neural Networks - A Review of Methods and Applications","text":"AbstractGraph neural networks are neural models that capture the dependence of graphs via message passing between the nodes of graphs. propose a general design pipeline for GNN models discuss the variants of each component systematically categorize the applications propose four open problems for future research IntroductionGraphs can be used across various areas social networks Graph Convolutional Networks with Markov Random Field Reasoning for Social Spammer Detection, 2020 natural science Graph Networks as Learnable Physics Engines for Inference and Control, 2018 Interaction Networks for Learning about Objects, Relations and Physics, 2016 protein-protein interaction networks Protein Interface Prediction using Graph Convolutional Networks, 2017 knowledge graphs Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach, 2017 other research areas Learning Combinatorial Optimization Algorithms over Graphs, 2017 The fundamental motivations of graph neural networks Recursive Neural Networks are first utilized on directed acyclic graphs Supervised neural networks for the classification of structures,1997 A general framework for adaptive processing of data structures, 1998 Recurent Neural Networks and Feedforward Neural Networks The Graph Neural Network Model, 2009 Neural Network for Graphs: A Contextual Constructive Approach, 2009 CNNs result in the rediscovery of GNNs Gradient-based learning applied to document recognition, 1998 the new era of deep learning Deep learning, 2015 geometric deep learning Geometric deep learning: going beyond Euclidean data, 2017 graph representation learning A Survey on Network Embedding, 2017Representation Learning on Graphs: Methods and Applications, 2017Network Representation Learning: A Survey, 2017A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications, 2017Graph embedding techniques, applications, and performance: A survey,2018 word representations Efficient Estimation of Word Representations in Vector Space, 2013 DeepWalk DeepWalk: Online Learning of Social Representations, 2014 SkipGram model Efficient Estimation of Word Representations in Vector Space, 2013 node2vec node2vec: Scalable Feature Learning for Networks, 2016 LINE LINE: Large-scale Information Network Embedding, 2015 TADW Network representation learning with rich text information, 2015 drawbacks First, no parameters are shared between nodes in the encoder, which leads to computationally inefficiency, since it means the number of parameters grows linearly with the number of nodes.Second, the direct embedding methods lack the ability of generalization, which means they cannot deal with dynamic graphs or generalize to new graphs. several comprehensive reviews on graph neural networks Geometric deep learning: going beyond Euclidean data, 2017Graph convolutional networks: a comprehensive review, 2019 the most up-to-date survey papers on GNNs Deep Learning on Graphs: A Survey, 2018A Comprehensive Survey on Graph Neural Networks, 2019Machine Learning on Graphs: A Model and Comprehensive Taxonomy, 2020 several surveys focusing on some specific graph learning fields adversarial learning methods on graphs Adversarial Attack and Defense on Graph Data: A Survey, 2018A Survey of Adversarial Learning on Graphs, 2020 a review over graph attention models Attention Models in Graphs: A Survey, 2018 heterogeneous graph representation learning Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond existing GNN models for dynamic graphs Modeling Complex Spatial Patterns with Temporal Features via Heterogenous Graph Embedding Networks, 2020 graph embeddings methods for combinatorial optimization. Graph Embedding for Combinatorial Optimization: A Survey contributions We provide a detailed review over existing graph neural network models. We present a general design pipeline and discuss the variants of each module. We also introduce researches on theoretical and empirical analyses of GNN models. We systematically categorize the applications and divide the applications into structural scenarios and non-structural scenarios. We present several major applications and their corresponding methods for each scenario. We propose four open problems for future research. We provide a thorough analysis of each problem and propose future research directions. General design pipeline of GNNsFind graph structure structural scenariosnon-structural scenarios Specify graph type and scale Directed/Undirected Graphs Homogeneous/Heterogeneous Graphs Static/Dynamic Graphs Design loss function Node-levelEdge-levelGraph-level Supervised settingSemi-supervised settingUnsupervised setting Build model using computational modules Propagation ModuleSampling ModulePooling Module NDCN(Neural Dynamics on Complex Networks) combines ordinary differential equation systems (ODEs) and GNNs. Instantiations of computational modules Propagation modules - convolution operatorThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains Spectral approaches \\mathscr F(\\mathbf x)=\\mathbf U^T\\mathbf x \\mathscr F^{-1}(\\mathbf x)=\\mathbf U\\mathbf x the normalized graph Laplacian \\mathbf L=\\mathbf I_N-\\mathbf D^{-\\frac{1}{2}}\\mathbf A\\mathbf D^{-\\frac{1}{2}} \\mathbf D\\ is\\ the\\ degree\\ matrix \\mathbf A is\\ the\\ adjacency\\ matrix\\ of\\ the\\ graphA wavelet tour of signal processing \\begin{aligned} \\mathbf g\\star\\mathbf x&=\\mathscr F^{-1}(\\mathscr F(\\mathbf g)\\odot\\mathscr F(\\mathbf x)\\\\ &=\\mathbf U(\\mathbf U^T\\mathbf g\\odot\\mathbf U^T\\mathbf x) \\end{aligned} \\mathbf U^T\\mathbf g\\ is\\ the\\ filter\\ in\\ the\\ spectral\\ domain \\mathbf g\\star\\mathbf x=\\mathbf U\\mathbf g_w\\mathbf U^T\\mathbf x several typical spectral methods which design different filters \\mathbf g_w Spectral Network Spectral Networks and Locally Connected Networks on Graphs Deep Convolutional Networks on Graph-Structured Data ChebNet Wavelets on Graphs via Spectral Graph Theory Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering GCN Basic spatial approaches Attention-based spatial approaches General frameworks for spatial approaches Propagation modules - recurrent operatorPropagation modules - skip connection Reference PaperGraph Neural Networks: A Review of Methods and Applications, 2018","link":"/2023/01/14/Graph%20Neural%20Networks%20-%20A%20Review%20of%20Methods%20and%20Applications/"},{"title":"Fusion","text":"Thermonuclear Fusion heat the deuterium-tritium fuel to a sufficienly high temperature Necessary temperature around 10keV, about 100 million degrees centigrade In a tokamak the plasma particles are confined to a toroidal region by a magnetic field, being held by the field in small gyrating orbits. Energy required for ignition\\hat n\\tau_E\\hat T>5\\times10^{21}m^{-3}sKeV\\hat n\\ is\\ the\\ peak\\ ion\\ density\\ in\\ the\\ plasma\\hat T\\ is\\ the\\ peak\\ temperature\\ in\\ the\\ plasma\\tau_E\\ is\\ the\\ energy\\ confinement\\ time The nuclei of deuterium and tritium fuse to produce an alpha particle with the release of a neutron \\begin{aligned} _1D^2\\ \\ \\ \\ \\ \\ +\\ \\ \\ \\ \\ \\ _1T^3\\longrightarrow _2H&e^4\\ \\ \\ \\ \\ \\ \\ \\ +&_0n^1\\\\ &| &| \\\\ &35MeV\\ \\ + &&141MeV=176MeV \\end{aligned} Reference BookTokamaks by John Wesson","link":"/2023/01/14/Fusion/"},{"title":"Creation and Annihilation Operators for Identical Particles","text":"General formalismThe state space of a system of N distinguishable particles \\varepsilon_N=\\varepsilon_1(1)\\otimes\\varepsilon_1(2)\\otimes..\\otimes\\varepsilon_1(N) The space of the completely symmetric states for bosons \\varepsilon_S(N) The projectors S_N=\\frac{1}{N!}\\sum_\\alpha P_\\alpha The space of the completely antisymmetric states for fermions \\varepsilon_A(N) The projectors A_N=\\frac{1}{N!}\\sum_\\alpha\\varepsilon_\\alpha P_\\alpha The P_\\alpha is the N! permutation operators for the N particles The \\varepsilon_\\alpha is the parity of P_\\alpha The subscript \\alpha distinguishes the different permutations of the N particles, and therefore take N! different values Fock states for identical bosons |n_i,n_j,..,n_l,..\\rangle\\\\=cS_N|1:u_i;2:u_i;..;n_i:u_i;\\ \\ n_i+1:u_j;..n_i+n_j:u_j;..\\rangle The subscripts i, j, k, l, ..denote different basis vectors {|u_i\\rangle} of the state space \\varepsilon_1 of a single particle","link":"/2023/01/15/Creation%20and%20Annihilation%20Operators%20for%20Identical%20Particles/"},{"title":"Basics of Math and Graph","text":"Linear Algebra Scalar Vector \\mathbf x=\\begin{bmatrix} x_1\\\\x_2\\\\ \\vdots\\\\x_n \\end{bmatrix} The L_p norm of a vector \\Vert\\mathbf x\\Vert_p=(\\sum_{i=1}^{n}\\lvert x_i\\rvert^p)^\\frac{1}{p} The L_1 norm of a vector \\Vert\\mathbf x\\Vert_1=\\sum_{i=1}^{n}\\lvert x_i\\rvert The L_2 norm of a vector \\Vert\\mathbf x\\Vert_2=\\sqrt{\\sum_{i=1}^{n}x_i^2} The L_\\infty norm of a vector \\Vert \\mathbf x\\Vert_\\infty=\\mathop{max}\\limits_{i}\\lvert x_i\\rvert The distance of two vectors \\mathbf x_1,\\mathbf x_2 \\mathbf D_p(\\mathbf x_1,\\mathbf x_2)=\\Vert\\mathbf x_1-\\mathbf x_2\\Vert_p A set of vectors \\mathbf x_1,\\mathbf x_2,\\cdots,\\mathbf x_m are linearly independent not exist a set of scalars \\lambda_1,\\lambda_2,\\cdots,\\lambda_m, which are not all 0 \\lambda_1\\mathbf x_1+\\lambda_2\\mathbf x_2+\\cdots+\\lambda_m\\mathbf x_m=0 Matrix \\mathbf A=\\begin{bmatrix} a_{11}&a_{12}&\\cdots&a_{1n}\\\\ a_{21}&a_{22}&\\cdots&a_{2n}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ a_{m1}&a_{m2}&\\cdots&a_{mn} \\end{bmatrix} Matrix product \\mathbf A\\in\\mathbb R^{m\\times n} \\mathbf B\\in\\mathbb R^{n\\times p} \\mathbf C_{ij}=\\sum_{k=1}^{n}\\mathbf A_{ik}\\mathbf B_{kj} \\mathbf C\\in\\mathbb R^{m\\times p} determinant det(\\mathbf A)=\\sum_{k_1k_2\\cdots k_n}(-1)^{\\tau(k_1k_2\\cdots k_n)} a_{1k_1}a_{2k_2}\\cdots a_{nk_n} k_1k_2\\cdots k_n\\ is\\ a\\ permutation\\ of\\ 1,2,\\cdots,n \\tau(k_1k_2\\cdots k_n)\\ is\\ the\\ inversion\\ number\\ of\\ the\\ permutationk_1k_2\\cdots k_n The inverse matrix \\mathbf A^{-1}\\mathbf A=\\mathbf I The transpose of matrix \\mathbf A_{ij}^{T}=\\mathbf A_{ji} The Hadamard product \\mathbf A\\in\\mathbb R^{m\\times n} \\mathbf B\\in\\mathbb R^{m\\times n} \\mathbf C_{ij}=\\mathbf A_{ij}\\mathbf B_{ij} \\mathbf C\\in\\mathbb R^{m\\times n} Tensor: An array with arbitrary dimension Probability theory joint probability P(X=x_1,Y=y_1) conditional probability P(X=x_1|Y=y_1) The sum rule P(X=x)=\\sum_yP(X=x,Y=y) The product rule P(X=x,Y=y)=P(Y=y|X=x)P(X=x) Bayes formula P(Y=y|X=x)=\\frac{P(X=x,Y=y)}{P(X=x)}=\\frac{P(X=x|Y=y)P(Y=y)}{P(X=x)} P(X_i=x_i|Y=y)=\\frac{P(Y=y|X_i=x_i)P(X_i=x_i)}{\\sum_{j=1}^nP(Y=y|X_j=x_j)} The chain rule P(X_1=x_1,\\cdots,X_n=x_n)\\\\ =P(X_1=x_1)\\prod_{i=2}^{n}P(X_i=x_i|X_1=x_1,\\cdots,X_{i-1}=x_{i-1}) The expectation of f(x) \\mathbb E[f(x)]=\\sum_xP(x)f(x) The variance of f(x) \\begin{aligned} Var(f(x)&=\\mathbb E[(f(x)-\\mathbb E[f(x)])^2]\\\\ &=\\mathbb E[f(x)^2]-\\mathbb E[f(x)]^2 \\end{aligned} The standard deviation \\sqrt{Var(f(x)} Covariance Cov(f(x),g(y))=\\mathbb E[(f(x)-\\mathbb E[f(x)])(g(y)-\\mathbb E[g(y)])] Gaussian distribution N(x|\\mu,\\sigma^2)=\\sqrt{\\frac{1}{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2) \\mu\\ is\\ the\\ mean\\ of\\ variable\\ x \\sigma^2\\ is\\ the\\ variance Bernoulli distribution P(X=x)=p^x(1-p)^{(1-x)},x \\in \\left\\{0,1\\right\\} Binomial distribution P(Y=k)=\\begin{pmatrix}N\\\\k\\end{pmatrix}p^k(1-p)^{(N-k)} Laplace distribution P(x|\\mu,b)=\\frac{1}{2b}exp(-\\frac{\\lvert x -\\mu\\rvert}{b}) Graph theory G=(V,E) V\\ is\\ the\\ set\\ of\\ vertices E\\ is\\ the\\ set\\ of\\ edges Adjacency matrix A_{ij}=\\left\\{ \\begin{aligned} &1\\ if\\left\\{v_i,\\ v_j\\right\\}\\in E\\ and\\ i\\ \\neq\\ j,\\\\ &0\\ otherwise. \\end{aligned} \\right. Degree matrix D_{ii}=d(v_i) Laplacian matrix L=D-A L_{ij}=\\left\\{ \\begin{aligned} &d(v_i)\\ if\\ i=j,\\\\ &-1\\ if\\left\\{v_i,v_j\\right\\}\\in E\\ and\\ i\\ \\neq\\ j,\\\\ &0\\ otherwise. \\end{aligned} \\right. Symmetric normalized Laplacian \\begin{aligned} L^{sym}&=D^{-\\frac{1}{2}}LD^{-\\frac{1}{2}}\\\\ &=I-D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}} \\end{aligned} L_{ij}^{sym}=\\left\\{ \\begin{aligned} &1\\ if\\ i=j\\ and\\ d(v_i)\\neq\\ 0,\\\\ &-\\frac{1}{\\sqrt{d(v_i)d(v_j)}}if\\left\\{v_i,v_j\\right\\}\\in E\\ and\\ i\\ \\neq\\ j,\\\\ &0\\ otherwise. \\end{aligned} \\right. Random walk normalized Laplacian L^{rw}=D^{-1}L=I-D^{-1}A L_{ij}^{rw}= \\left\\{ \\begin{aligned} &1\\ if\\ i=j\\ and\\ d(v_i)\\ \\neq\\ 0,\\\\ &-\\frac{1}{d(v_i)}\\ if\\left\\{v_i,v_j\\right\\}\\in E\\ and\\ i\\ \\neq\\ j,\\\\ &0\\ otherwise. \\end{aligned} \\right. Incidence matrix For a directed graph M_{ij}= \\left\\{ \\begin{aligned} &1\\ if\\ \\exists\\ k\\ s.t\\ e_j=\\left\\{v_i,v_k\\right\\},\\\\ &-1\\ if\\ \\exists\\ k\\ s.t\\ e_j=\\left\\{v_k,v_i\\right\\},\\\\ &0\\ otherwise. \\end{aligned} \\right.For a undirected graph M_{ij}= \\left\\{ \\begin{aligned} &1\\ if\\ \\exists\\ k\\ s.t\\ e_j=\\left\\{v_i,v_k\\right\\},\\\\ &0\\ otherwise. \\end{aligned} \\right.","link":"/2023/01/19/Basics%20of%20Math%20and%20Graph/"},{"title":"Unique Factorization","text":"The prime number theory \\lim_{x\\to\\infty}\\frac{\\pi(x)}{x/\\ln(x)}=1 \\pi(x)\\ is\\ the\\ number\\ of\\ primes\\ between\\ 1\\ and\\ xLemma 1Every nonzero integer can be written as a product of primes. Theorem 1For every nonzero integer n there is a prime factorization n=(-1)^{\\epsilon(n)} \\prod_{p}p^{a(p)}with the exponents uniquely determined by n. a(p)=ord_{p}nLemma 2Ifa,b\\in \\mathbb{Z} and b&gt;0, there exist q,r\\in\\mathbb{Z}such that a=qb+r with 0\\leq r","link":"/2023/01/20/Unique%20Factorization/"},{"title":"Introduction to Complex Analysis","text":"holomorphicityA function f:\\mathbb C\\rightarrow\\mathbb C is holomorphic at the point z\\in\\mathbb C if the limit \\lim_{h\\to 0}\\frac{f(z+h)-f(z)}{h}(h\\in\\mathbb C)exists. characteristic properties of holomorphic functions Contour integration: If f is holomorphic in \\Omega,then for appropriate closed paths in \\Omega \\int_\\gamma f(z)dz=0 Regularity: If f is holomorphic, then f is indefinitely differentiable. Analytic continuation: If f and g are holomorphic functions in \\Omega which are equal in an arbitrarily small disc in \\Omega,then f=g everywhere in \\Omega. The zeta function \\zeta(s)=\\sum_{n=1}^{\\infty}\\frac{1}{n^s} The theta function \\Theta(z|\\tau)=\\sum_{n=-\\infty}^{\\infty}e^{\\pi in^2}e^{2\\pi inz} Basic properties The complex planeCommutativityAssociativityDistributivityThe absolute valueThe triangle inequalityThe complex conjugateIn polar form","link":"/2023/01/20/Introduction%20to%20Complex%20Analysis/"},{"title":"Nonuniform B and E Field","text":"Nonuniform B Field \\nabla B\\perp B:\\ Grad-B\\ Drift F_y=-qv_x\\mathbf B_z(y) =-qv_\\perp(cos\\omega_ct)[B_0\\pm r_L(cos\\omega_ct)\\frac{\\partial B}{\\partial y}] \\overline{\\mathbf F}_y=\\mp qv_\\perp r_L\\frac{1}{2}(\\partial\\mathbf B/\\partial y)The guiding center drift velocity \\mathbf v_{gc}=\\frac{1}{q}\\frac{\\mathbf F\\times\\mathbf B}{\\mathbf B^2}=\\frac{1}{q}\\frac{\\overline{F_y}}{\\lvert\\mathbf B\\rvert}\\hat{\\mathbf x}=\\mp\\frac{v_\\perp r_L}{\\mathbf B}\\frac{1}{2}\\frac{\\partial\\mathbf B}{\\partial y}\\hat{\\mathbf x}The\\ grad-\\mathbf B\\ drift \\mathbf v_{\\nabla B}=\\pm\\frac{1}{2}v_\\perp r_L\\frac{\\mathbf B\\times\\nabla\\mathbf B}{\\mathbf B^2} Curved B: Curvature Drift The centrifugal force \\mathbf F_{cf}=\\frac{m{v_{||}}^2}{\\mathbf R_c}\\hat{\\mathbf r}=m{v_{||}}^2\\frac{\\mathbf R_c}{\\mathbf R_c^2}The curvature drift velocity \\mathbf v_R=\\frac{1}{q}\\frac{\\mathbf F_{cf}\\times\\mathbf B}{\\mathbf B^2}=\\frac{mv_{||}^2}{q\\mathbf B^2}\\frac{\\mathbf R_c\\times\\mathbf B}{\\mathbf R_c^2}The total drift \\mathbf v_R+\\mathbf v_{\\nabla B}=\\frac{m}{q}\\frac{\\mathbf R_c\\times\\mathbf B}{\\mathbf R_c^2\\mathbf B^2}(v_{||}^2+\\frac{1}{2}v_\\perp^2)\\nabla B||B:\\ Magnetic\\ Mirrors","link":"/2023/01/23/Nonuniform%20B%20and%20E%20Field/"},{"title":"Uniform E and B Fields","text":"The conditions: E = 0 The equation of motionm\\frac{dv}{dt}=q\\mathbf{v}\\times\\mathbf{B}Cyclotron frequency\\omega_c\\equiv\\frac{\\lvert q\\rvert\\mathbf{B}}{m} v_x=v_\\perp e^{i\\omega_c t}v_y=\\pm iv_\\perp e^{i\\omega_c t} combine The Larmor radiusr_L\\equiv\\frac{v_\\perp}{\\omega_c}=\\frac{mv_\\perp}{\\lvert q\\rvert\\mathbf{B}}we get x-x_0=r_Lsin\\omega_c ty-y_0=\\pm r_L cos\\omega_c t guiding center (x_0, y_0) plasmas are diamagnetic The conditions: finite E The equation of motionm\\frac{d\\mathbf v}{dt}=q(\\mathbf E+\\mathbf v\\times\\mathbf B) v_x=v_\\perp e^{i\\omega_c t}v_y=\\pm iv_\\perp e^{i\\omega_c t}-\\frac{\\mathbf E_x}{\\mathbf B} The usual circular Larmor gyration A drift of the guiding center The three-dimensional orbit in space is therefore a slanted helix with changing pitch. The conditions: gravitational field \\mathbf v_g=\\frac{m}{q}\\frac{\\mathbf g\\times\\mathbf B}{\\mathbf B^2} The magnitude of v_g is usually negligible But an effective gravitational force due to centrifugal force is not negligible Reference BookIntroduction to Plasma Physics and Controlled Fusion by Francis F. Chen","link":"/2023/01/12/Uniform%20E%20and%20B%20Fields/"},{"title":"The Realm of Supervised Learning","text":"Preprocessing data using different techniquesGetting ready1234import numpy as npfrom sklearn import preprocessing data = np.array([[3, -1.5, 2, -5.4], [0, 4, -0.3, 2.1], [1, 3.3, -1.9, -4.3]]) ~~~~ How to do it…1234# Mean removaldata_standardized = preprocessing.scale(data)print &quot;\\nMean =&quot;, data_standardized.mean(axis=0)print &quot;Std deviation =&quot;, data_standardized.std(axis=0) python preprocessor.py Mean = [ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17] Std deviation = [ 1. 1. 1. 1.] 1234# Scaling data_scaler = preprocessing.MinMaxScaler(feature_range=(0.1))data_scaled = data_scaler.fit_transform(data)print &quot;\\nMin max scaled data =&quot;, data_scaled 123# Normalizationdata_normalized = preprocessing.normalize(data, norm='l1')print &quot;\\nL1 normalized data =&quot;, data_normalized 123# Binarizationdata_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)print &quot;\\nBinarized data =&quot;, data_binarized &gt; 12345# One Hot Encodingencoder = preprocessing.OneHotEncoder()encoder.fit([[0, 2, 1, 12], [1, 3, 5, 3], [2, 3, 2, 12],[1, 2, 4, 3]])encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()print &quot;\\nEncoded vector =&quot;, encoded_vector Label encodingHow to do it…123456from sklearn import preprocessinglabel_encoder = preprocessing.LabelEncoder()input_classes = ['audi', 'ford', 'audi', 'toyota', 'ford', 'bmw']label_encoder.fit(input_classes)print &quot;\\nClass mapping:&quot;for i, item in enumerate(label_encoder.classes_): print item, '--&gt;', i &gt; 1labels =","link":"/2023/01/23/The%20Realm%20of%20Supervised%20Learning/"},{"title":"Machine Learning for Symmetries and Conservation Laws","text":"Interplay between ML and PhysicsGoverning equantionConservation lawSymmetry Importance of Conservation and symmetryOverview AI Poincare 1D Harmonic Oscillator 2D Kepler Problem Conservation Law &amp; Manifold","link":"/2023/01/23/Machine%20Learning%20for%20Symmetries%20and%20Conservation%20Laws/"},{"title":"Special Relativity","text":"Coordinatizations of spacetimeIntuitively, an “event” is “something which happens at a definite place at a definite time”. The set \\mathbf E of all events is called spacetime. coordinatization of \\mathbf EE onto the four-dimensional real vector space R^4 e(t_e,x_e,y_e,z_e) t_e:=t_0-\\frac{1}{c}(x_e^2+y_e^2+z_e^2)^{1/2}Lorentz coordinatizationsProperties (1)For any stationary standard clock with coordinates (t(\\tau),x_0,y_0,z_0),t(\\tau)=\\tau for all \\tau. (2)Light always moves in straight lines with unit velocity.The function t\\longmapsto\\overrightarrow{r}(t) is of the form \\overrightarrow{r}(t)=\\overrightarrow{v}t+\\overrightarrow{r_0}, where \\overrightarrow{v},\\overrightarrow{r_0}\\in R^3 are constant vectors, and \\overrightarrow{v} is a unit vector. Minkowski space u=(u^0,u^1,u^2,u^3) v=(v^0,v^1,v^2,v^3) Minkowski metric :=u^0v^0-u^1v^1-u^2v^2-u^3v^3 Lorentz transformations","link":"/2023/01/23/Special%20Relativity/"}],"tags":[],"categories":[{"name":"PLASMA","slug":"PLASMA","link":"/categories/PLASMA/"},{"name":"TOKAMAKS","slug":"TOKAMAKS","link":"/categories/TOKAMAKS/"},{"name":"QUANTUM MECHANICS","slug":"QUANTUM-MECHANICS","link":"/categories/QUANTUM-MECHANICS/"},{"name":"ELECTRODYNAMICS","slug":"ELECTRODYNAMICS","link":"/categories/ELECTRODYNAMICS/"},{"name":"ARITHMETICS","slug":"ARITHMETICS","link":"/categories/ARITHMETICS/"},{"name":"COMPUTER VISION","slug":"COMPUTER-VISION","link":"/categories/COMPUTER-VISION/"},{"name":"PHYSICS WITH MACHINE LEARNING","slug":"PHYSICS-WITH-MACHINE-LEARNING","link":"/categories/PHYSICS-WITH-MACHINE-LEARNING/"}],"pages":[]}